{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from opensearchpy import OpenSearch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "model_card = 'sentence-transformers/msmarco-distilbert-base-tas-b'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection opened...\n"
     ]
    }
   ],
   "source": [
    "host = '3.145.52.195' #host = 'localhost' \n",
    "port = 9200\n",
    "auth =('admin','IVIngi2024!') #auth = ('admin','admin') \n",
    "client_lexical = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    "    timeout=500, \n",
    "    max_retries=1\n",
    "    #connection_class=RequestsHttpConnection \n",
    "#    http_compress = True, # enables gzip compression for request bodies\n",
    "#    use_ssl = False,\n",
    "#   verify_certs = False,\n",
    "#    ssl_assert_hostname = False,\n",
    "#    ssl_show_warn = False\n",
    ")\n",
    "print(\"Connection opened...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 31, 'timed_out': False, '_shards': {'total': 4, 'successful': 4, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 10000, 'relation': 'gte'}, 'max_score': 18.928892, 'hits': [{'_index': 'medline-faiss-hnsw-lexical', '_id': '2298154', '_score': 18.928892, '_source': {'pmid': '15868453', 'auto_id': 2298154, 'text': \"What do cancer survivors believe causes cancer? (United States). OBJECTIVE To describe cancer survivors' beliefs about the causes of prostate, colorectal or breast cancers. METHODS A survey of beliefs about cancer causation was completed by 670 cancer survivors (416 with breast cancer, 165 with prostate cancer and 89 with colorectal cancer) enrolled in a population-based study in Colorado. Categorical analysis was used to describe associations between participant's beliefs about the cause of their cancer type, both in themselves and in others, and personal characteristics, including gender, age, and familial cancer risk. RESULTS Cancer survivors most frequently reported genetic factors, smoking, environmental factors (e.g., pollutants or occupation), and psychosocial factors (e.g., stress) as causing their type of cancer. Respondents underestimated the importance of behavioral factors that are known to be associated with increased cancer risk, such as obesity and physical inactivity, while overestimating the importance of stress and environmental pollution. CONCLUSIONS Cancer survivors' beliefs about what causes cancer are substantially different than those of experts. Because those affected by cancer should be well informed about the causes of cancer, educational efforts are needed, especially regarding the importance of factors that can be modified to reduce cancer risk.\", 'full_text': \"What do cancer survivors believe causes cancer? (United States).\\n\\nOBJECTIVE\\nTo describe cancer survivors' beliefs about the causes of prostate, colorectal or breast cancers.\\n\\n\\nMETHODS\\nA survey of beliefs about cancer causation was completed by 670 cancer survivors (416 with breast cancer, 165 with prostate cancer and 89 with colorectal cancer) enrolled in a population-based study in Colorado. Categorical analysis was used to describe associations between participant's beliefs about the cause of their cancer type, both in themselves and in others, and personal characteristics, including gender, age, and familial cancer risk.\\n\\n\\nRESULTS\\nCancer survivors most frequently reported genetic factors, smoking, environmental factors (e.g., pollutants or occupation), and psychosocial factors (e.g., stress) as causing their type of cancer. Respondents underestimated the importance of behavioral factors that are known to be associated with increased cancer risk, such as obesity and physical inactivity, while overestimating the importance of stress and environmental pollution.\\n\\n\\nCONCLUSIONS\\nCancer survivors' beliefs about what causes cancer are substantially different than those of experts. Because those affected by cancer should be well informed about the causes of cancer, educational efforts are needed, especially regarding the importance of factors that can be modified to reduce cancer risk.\", 'authors': [{'lastname': 'Wold', 'forename': 'Kimberly S', 'initials': 'KS', 'identifier': '', 'affiliation': 'Division of Cancer Prevention and Control, University of Colorado Cancer Center, Denver, CO, USA.'}, {'lastname': 'Byers', 'forename': 'Tim', 'initials': 'T', 'identifier': '', 'affiliation': ''}, {'lastname': 'Crane', 'forename': 'Lori A', 'initials': 'LA', 'identifier': '', 'affiliation': ''}, {'lastname': 'Ahnen', 'forename': 'Dennis', 'initials': 'D', 'identifier': '', 'affiliation': ''}], 'journal': 'Cancer causes & control : CCC', 'pubdate': '2005-03'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '2090351', '_score': 18.928036, '_source': {'pmid': '15613283', 'auto_id': 2090351, 'text': 'Multiple numerical chromosome aberrations in cancer: what are their causes and what are their consequences? Several neoplastic tumor types are cytogenetically characterized by multiple numerical chromosome abnormalities without concomitant structural karyotypic changes. At present, no good gene-level theories are at hand to explain the pathogenetic effect of these changes during tumorigenesis, nor is it known how they arise or what causes them. Genetic instability is often invoked as an underlying cause, but actual data favoring this explanation are meager or non-existing. Numerical chromosome changes and ploidy shifts allow the simultaneous alteration of multiple cancer-relevant genes, thereby reducing the number of independent genomic events necessary for carcinogenesis and the need for postulating genomic instability as a necessity in cancer development.', 'full_text': 'Multiple numerical chromosome aberrations in cancer: what are their causes and what are their consequences?\\n\\nSeveral neoplastic tumor types are cytogenetically characterized by multiple numerical chromosome abnormalities without concomitant structural karyotypic changes. At present, no good gene-level theories are at hand to explain the pathogenetic effect of these changes during tumorigenesis, nor is it known how they arise or what causes them. Genetic instability is often invoked as an underlying cause, but actual data favoring this explanation are meager or non-existing. Numerical chromosome changes and ploidy shifts allow the simultaneous alteration of multiple cancer-relevant genes, thereby reducing the number of independent genomic events necessary for carcinogenesis and the need for postulating genomic instability as a necessity in cancer development.', 'authors': [{'lastname': 'Teixeira', 'forename': 'Manuel R', 'initials': 'MR', 'identifier': '', 'affiliation': 'Department of Genetics, Portuguese Oncology Institute, Rua Dr. AntÃ³nio Bernardino de Almeida, 4200-072 Porto, Portugal. mteixeir@ipoporto.min-saude.pt'}, {'lastname': 'Heim', 'forename': 'Sverre', 'initials': 'S', 'identifier': '', 'affiliation': ''}], 'journal': 'Seminars in cancer biology', 'pubdate': '2005-02'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '14805642', '_score': 18.776869, '_source': {'pmid': '31345063', 'auto_id': 14805642, 'text': 'What Caused My Cancer? Cancer Patients\\' Perceptions on What May Have Contributed to the Development of Their Cancer: A Cross-Sectional, Cross-Country Comparison Study. Accurate public perceptions on the risk factors associated with cancer are important in promoting primary, secondary, and tertiary prevention. Limited studies have explored this topic among patients with cancer in non-western, low-to-middle-income countries. A cross-sectional survey to compare Australian and Vietnamese cancer patients\\' perceptions of what caused their cancer was undertaken. Adult, patients with cancer from both countries, receiving radiotherapy treatment completed a standardized survey, which included a 25-item module assessing their beliefs on the causes of their cancer. Items ranged from known evidence-based causes (eg, smoking, sun exposure) to non-evidence-based beliefs (eg, stress or anxiety, physical injury, or trauma). Country-specific logistic regression analyses were conducted to identify differences in the determinants of patients\\' top perceived causes. A total of 585 patient surveys were completed (75% response rate; 285 from Australia, and 300 from Vietnam). Most patients were male (58%) and aged 60 years and older (55%). The most frequently reported risk factor overall and for the Australian sample was \"getting older\" (overall = 42%, Australia = 49%, and Vietnam = 35%). While the most frequently reported risk factor for the Vietnamese sample was \"poor diet\" (overall = 39%, Australia = 11%, and Vietnam = 64%). There were differences in the characteristics associated with the top causes of cancer identified by Australian and Vietnamese patients. Patients\\' beliefs about what may have caused their cancer are complex and likely to be impacted by multiple factors, including the country from which they reside. Developing public awareness campaigns that are accurate and tailored to address the specific beliefs and possible misconceptions held by the target community are needed.', 'full_text': 'What Caused My Cancer? Cancer Patients\\' Perceptions on What May Have Contributed to the Development of Their Cancer: A Cross-Sectional, Cross-Country Comparison Study.\\n\\nAccurate public perceptions on the risk factors associated with cancer are important in promoting primary, secondary, and tertiary prevention. Limited studies have explored this topic among patients with cancer in non-western, low-to-middle-income countries. A cross-sectional survey to compare Australian and Vietnamese cancer patients\\' perceptions of what caused their cancer was undertaken. Adult, patients with cancer from both countries, receiving radiotherapy treatment completed a standardized survey, which included a 25-item module assessing their beliefs on the causes of their cancer. Items ranged from known evidence-based causes (eg, smoking, sun exposure) to non-evidence-based beliefs (eg, stress or anxiety, physical injury, or trauma). Country-specific logistic regression analyses were conducted to identify differences in the determinants of patients\\' top perceived causes. A total of 585 patient surveys were completed (75% response rate; 285 from Australia, and 300 from Vietnam). Most patients were male (58%) and aged 60 years and older (55%). The most frequently reported risk factor overall and for the Australian sample was \"getting older\" (overall = 42%, Australia = 49%, and Vietnam = 35%). While the most frequently reported risk factor for the Vietnamese sample was \"poor diet\" (overall = 39%, Australia = 11%, and Vietnam = 64%). There were differences in the characteristics associated with the top causes of cancer identified by Australian and Vietnamese patients. Patients\\' beliefs about what may have caused their cancer are complex and likely to be impacted by multiple factors, including the country from which they reside. Developing public awareness campaigns that are accurate and tailored to address the specific beliefs and possible misconceptions held by the target community are needed.', 'authors': [{'lastname': 'Hall', 'forename': 'Alix', 'initials': 'A', 'identifier': 'https://orcid.org/0000-0002-1043-6110', 'affiliation': '1 Priority Research Centre for Health Behaviour, Faculty of Health, The University of Newcastle & Hunter Medical Research Institute, Callaghan, New South Wales, Australia.'}, {'lastname': 'Nguyen', 'forename': 'Sang Minh', 'initials': 'SM', 'identifier': '', 'affiliation': '2 Division of Epidemiology, Department of Medicine, Vanderbilt University School of Medicine, Nashville, TN, USA.'}, {'lastname': 'Mackenzie', 'forename': 'Lisa', 'initials': 'L', 'identifier': '', 'affiliation': '1 Priority Research Centre for Health Behaviour, Faculty of Health, The University of Newcastle & Hunter Medical Research Institute, Callaghan, New South Wales, Australia.'}, {'lastname': 'Sanson-Fisher', 'forename': 'Rob', 'initials': 'R', 'identifier': '', 'affiliation': '1 Priority Research Centre for Health Behaviour, Faculty of Health, The University of Newcastle & Hunter Medical Research Institute, Callaghan, New South Wales, Australia.'}, {'lastname': 'Olver', 'forename': 'Ian', 'initials': 'I', 'identifier': '', 'affiliation': '3 University of South Australia Cancer Research Institute, Adelaide, Australia.'}, {'lastname': 'Thuan', 'forename': 'Tran Van', 'initials': 'TV', 'identifier': '', 'affiliation': '4 National Cancer Hospital, National Cancer Institute, Hanoi, Vietnam.'}, {'lastname': 'Huong', 'forename': 'Tran Thanh', 'initials': 'TT', 'identifier': '', 'affiliation': '5 Vietnam National Cancer Institute, Hanoi Medical University, Hanoi, Vietnam.'}], 'journal': 'Cancer control : journal of the Moffitt Cancer Center', 'pubdate': '2019'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '21704390', '_score': 17.92782, '_source': {'pmid': '38282044', 'auto_id': 21704390, 'text': \"What do cancer survivors believe caused their cancer? A secondary analysis of cross-sectional survey data. PURPOSE Given that risk reduction and healthy lifestyles can prevent 4 in 10 cancers, it is important to understand what survivors believe caused their cancer to inform educational initiatives. METHODS In this secondary analysis, we analyzed cancer survivor responses on the Causes Subscale of the Revised Illness Perception Questionnaire, which lists 18 possible causes of illness and a free text question. We used descriptive statistics to determine cancer survivors' agreement with the listed causes and conducted separate partial proportional odds models for the top three causes to examine their associations with sociodemographic and clinical characteristics. Content analysis was used to examine free text responses. RESULTS Of the 1,001 participants, most identified as Caucasian (n\\u2009=\\u2009764, 77%), female (n\\u2009=\\u2009845, 85%), and were diagnosed with breast cancer (n\\u2009=\\u2009656, 66%). The most commonly believed causes of cancer were: stress or worry (n\\u2009=\\u2009498, 51%), pollution in the environment (n\\u2009=\\u2009471, 48%), and chance or bad luck (n\\u2009=\\u2009412, 42%). The associations of sociodemographic and clinical variables varied across the models. Free text responses indicated that hereditary and genetic causes (n\\u2009=\\u2009223, 22.3%) followed by trauma and stress (n\\u2009=\\u2009218, 21.8%) and bad luck or chance (n\\u2009=\\u200979, 7.9%) were the most important causes of cancer. CONCLUSIONS Study results illuminate cancer survivors' beliefs about varying causes of their cancer diagnosis and identify characteristics of survivors who are more likely to believe certain factors caused their cancer. Results can be used to plan cancer education and risk-reduction campaigns and highlight for whom such initiatives would be most suitable.\", 'full_text': \"What do cancer survivors believe caused their cancer? A secondary analysis of cross-sectional survey data.\\n\\nPURPOSE\\nGiven that risk reduction and healthy lifestyles can prevent 4 in 10 cancers, it is important to understand what survivors believe caused their cancer to inform educational initiatives.\\n\\n\\nMETHODS\\nIn this secondary analysis, we analyzed cancer survivor responses on the Causes Subscale of the Revised Illness Perception Questionnaire, which lists 18 possible causes of illness and a free text question. We used descriptive statistics to determine cancer survivors' agreement with the listed causes and conducted separate partial proportional odds models for the top three causes to examine their associations with sociodemographic and clinical characteristics. Content analysis was used to examine free text responses.\\n\\n\\nRESULTS\\nOf the 1,001 participants, most identified as Caucasian (n\\u2009=\\u2009764, 77%), female (n\\u2009=\\u2009845, 85%), and were diagnosed with breast cancer (n\\u2009=\\u2009656, 66%). The most commonly believed causes of cancer were: stress or worry (n\\u2009=\\u2009498, 51%), pollution in the environment (n\\u2009=\\u2009471, 48%), and chance or bad luck (n\\u2009=\\u2009412, 42%). The associations of sociodemographic and clinical variables varied across the models. Free text responses indicated that hereditary and genetic causes (n\\u2009=\\u2009223, 22.3%) followed by trauma and stress (n\\u2009=\\u2009218, 21.8%) and bad luck or chance (n\\u2009=\\u200979, 7.9%) were the most important causes of cancer.\\n\\n\\nCONCLUSIONS\\nStudy results illuminate cancer survivors' beliefs about varying causes of their cancer diagnosis and identify characteristics of survivors who are more likely to believe certain factors caused their cancer. Results can be used to plan cancer education and risk-reduction campaigns and highlight for whom such initiatives would be most suitable.\", 'authors': [{'lastname': 'Galica', 'forename': 'Jacqueline', 'initials': 'J', 'identifier': '', 'affiliation': \"Queen's University School of Nursing, 92 Barrie Street, Kingston, ON, K7L 3N6, Canada. jacqueline.galica@queensu.ca.\"}, {'lastname': 'Saunders', 'forename': 'Stephanie', 'initials': 'S', 'identifier': '', 'affiliation': 'The Ottawa Hospital, 1053 Carling Avenue, Ottawa, ON, K1Y 4E9, Canada.'}, {'lastname': 'Pan', 'forename': 'Ziwei', 'initials': 'Z', 'identifier': '', 'affiliation': \"Department of Mathematics and Statistics, Queen's University, 48 University Avenue, Kingston, ON, K7L 3N6, Canada.\"}, {'lastname': 'Silva', 'forename': 'Amina', 'initials': 'A', 'identifier': '', 'affiliation': 'Faculty of Applied Health Sciences, Brock University, 1812 Sir Isaac Brock Way, St. Catharines, ON, L2S 3A1, Canada.'}, {'lastname': 'Ling', 'forename': 'Hok Kan', 'initials': 'HK', 'identifier': '', 'affiliation': \"Department of Mathematics and Statistics, Queen's University, 48 University Avenue, Kingston, ON, K7L 3N6, Canada.\"}], 'journal': 'Cancer causes & control : CCC', 'pubdate': '2024-01-28'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '1628440', '_score': 17.860188, '_source': {'pmid': '14981975', 'auto_id': 1628440, 'text': 'Cancer cachexia: what is known about its etiology and what should be the current treatment approach? Cancer cachexia, defined as involuntary weight loss and tissue wasting due to cancer, negatively influences physical condition, quality of life and prognosis. Well known causes, such as ileus or hypercalcemia, do not suffice to explain the entire phenomenon. Metabolic changes induced by the tumor and/or host are supposed to play a deciding role. In the present review current insights into the etiology and treatment are discussed.', 'full_text': 'Cancer cachexia: what is known about its etiology and what should be the current treatment approach?\\n\\nCancer cachexia, defined as involuntary weight loss and tissue wasting due to cancer, negatively influences physical condition, quality of life and prognosis. Well known causes, such as ileus or hypercalcemia, do not suffice to explain the entire phenomenon. Metabolic changes induced by the tumor and/or host are supposed to play a deciding role. In the present review current insights into the etiology and treatment are discussed.', 'authors': [{'lastname': 'van Halteren', 'forename': 'H K', 'initials': 'HK', 'identifier': '', 'affiliation': 'Department of Internal Medicine, Oosterschelde Hospital, PO Box 106, 4460 BB Goes, The Netherlands. Hvanhalteren@soz.nl'}, {'lastname': 'Bongaerts', 'forename': 'G P', 'initials': 'GP', 'identifier': '', 'affiliation': ''}, {'lastname': 'Wagener', 'forename': 'D J', 'initials': 'DJ', 'identifier': '', 'affiliation': ''}], 'journal': 'Anticancer research', 'pubdate': '2003'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '27470363', '_score': 17.638792, '_source': {'pmid': '10664443', 'auto_id': 27470363, 'text': 'Cellular and molecular basis of preferential metastasis of breast cancer to bone. Bone is one of the most preferential target sites for cancer metastasis. Breast cancer has a predilection for spreading to bone, and bone metastasis is one of the major causes of increased morbidity and eventual mortality in breast cancer patients. None of the currently available therapies is effective for curing bone metastases in these patients. Elucidation of the cellular and molecular mechanism by which breast cancer selectively spreads to bone is essential for the development of mechanism-based effective and specific therapeutic interventions for this deleterious complication in breast cancer. Here, two questions are addressed to study the mechanism of breast cancer metastasis to bone: (1) What makes bone a preferential target site of metastasis? (2) What makes breast cancer able to colonize bone? (3) An animal model in which intracardiac inoculation of breast cancer cells selectively causes osteolytic bone metastases was developed. Experimental results obtained using this unique in-vivo model of bone metastasis are described and discussed.', 'full_text': 'Cellular and molecular basis of preferential metastasis of breast cancer to bone.\\n\\nBone is one of the most preferential target sites for cancer metastasis. Breast cancer has a predilection for spreading to bone, and bone metastasis is one of the major causes of increased morbidity and eventual mortality in breast cancer patients. None of the currently available therapies is effective for curing bone metastases in these patients. Elucidation of the cellular and molecular mechanism by which breast cancer selectively spreads to bone is essential for the development of mechanism-based effective and specific therapeutic interventions for this deleterious complication in breast cancer. Here, two questions are addressed to study the mechanism of breast cancer metastasis to bone: (1) What makes bone a preferential target site of metastasis? (2) What makes breast cancer able to colonize bone? (3) An animal model in which intracardiac inoculation of breast cancer cells selectively causes osteolytic bone metastases was developed. Experimental results obtained using this unique in-vivo model of bone metastasis are described and discussed.', 'authors': [{'lastname': 'Yoneda', 'forename': 'T', 'initials': 'T', 'identifier': '', 'affiliation': 'Department of Biochemistry, Osaka University Faculty of Dentistry, 1-8 Yamada-oka, Suita, Osaka 565-0891, Japan.'}], 'journal': 'Journal of orthopaedic science : official journal of the Japanese Orthopaedic Association', 'pubdate': '2000'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '15774130', '_score': 17.297037, '_source': {'pmid': '32391946', 'auto_id': 15774130, 'text': 'Why do young people get cancer? Oncologists and cancer biologists are frequently confronted by the question of what causes cancer? This is particularly vexing for cancers affecting children and young adults who have had limited exposure to environmental mutagens and the effects of aging. Here, I focus on a general framework of the causes of early-onset cancer development in children and young adults by relating inherited and constitutional cancer predisposition, oncogenic pathogens, and developmental mutations. This framework has implications not only for mechanistic investigation of young cancers, but should also clarify improved strategies for their treatment, screening, and potential prevention.', 'full_text': 'Why do young people get cancer?\\n\\nOncologists and cancer biologists are frequently confronted by the question of what causes cancer? This is particularly vexing for cancers affecting children and young adults who have had limited exposure to environmental mutagens and the effects of aging. Here, I focus on a general framework of the causes of early-onset cancer development in children and young adults by relating inherited and constitutional cancer predisposition, oncogenic pathogens, and developmental mutations. This framework has implications not only for mechanistic investigation of young cancers, but should also clarify improved strategies for their treatment, screening, and potential prevention.', 'authors': [{'lastname': 'Kentsis', 'forename': 'Alex', 'initials': 'A', 'identifier': '0000-0002-8063-9191', 'affiliation': 'Sloan Kettering Institute and Department of Pediatrics, Weill Medical College of Cornell University and Memorial Sloan Kettering Cancer Center, New York, New York.'}], 'journal': 'Pediatric blood & cancer', 'pubdate': '2020-07'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '5573064', '_score': 17.178644, '_source': {'pmid': '20207672', 'auto_id': 5573064, 'text': \"Answering patient questions about the role lifestyle factors play in cancer onset and recurrence: what do health care professionals say? This qualitative study examined how cancer specialists answer patient questions about what might have caused their cancer. Findings showed that while they were often candid about the role of smoking and drinking in cancer onset and that of diet in cancer recurrence, body weight and exercise were rarely mentioned. Any reluctance to discuss the role of lifestyle factors in cancer onset and recurrence arose from a desire to minimize patient distress, limitations in specialists' knowledge of the causes of cancer and perceived inadequacy of the available causal explanations when risk factors are multiple and probabilistic.\", 'full_text': \"Answering patient questions about the role lifestyle factors play in cancer onset and recurrence: what do health care professionals say?\\n\\nThis qualitative study examined how cancer specialists answer patient questions about what might have caused their cancer. Findings showed that while they were often candid about the role of smoking and drinking in cancer onset and that of diet in cancer recurrence, body weight and exercise were rarely mentioned. Any reluctance to discuss the role of lifestyle factors in cancer onset and recurrence arose from a desire to minimize patient distress, limitations in specialists' knowledge of the causes of cancer and perceived inadequacy of the available causal explanations when risk factors are multiple and probabilistic.\", 'authors': [{'lastname': 'Miles', 'forename': 'Anne', 'initials': 'A', 'identifier': '', 'affiliation': 'Department of Epidemiology and Public Health, University College London, London WC1E 6BT, UK. a.miles@ucl.ac.uk'}, {'lastname': 'Simon', 'forename': 'Alice', 'initials': 'A', 'identifier': '', 'affiliation': ''}, {'lastname': 'Wardle', 'forename': 'Jane', 'initials': 'J', 'identifier': '', 'affiliation': ''}], 'journal': 'Journal of health psychology', 'pubdate': '2010-03'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '16687968', '_score': 17.162868, '_source': {'pmid': '33369797', 'auto_id': 16687968, 'text': 'Bone metastases induce metabolic changes and mitophagy in mice. NEW FINDINGS What is the central question of this study? Cachexia causes severe changes in skeletal muscle metabolism and function and is a key predictor of negative outcomes in cancer patients: what are the changes in whole animal energy metabolism and mitochondria in skeletal muscle? What is the main finding and its importance? There is decreased whole animal energy expenditure in mice with cachexia. They displayed highly dysmorphic mitochondria and mitophagy in skeletal muscle. ABSTRACT Cachexia causes changes in skeletal muscle metabolism. Mice with MDA-MB-231 breast cancer bone metastases and cachexia have decreased whole animal energy metabolism and increased skeletal muscle mitophagy. We examined whole animal energy metabolism by indirect calorimetry in mice with MDA-MB-231 breast cancer bone metastases, and showed decreased energy expenditure. We also examined skeletal muscle mitochondria and found that mitochondria in mice with MDA-MB-231 bone metastases are highly dysmorphic and have altered protein markers of mitochondrial biogenesis and dynamics. In addition, LC3B protein was increased in mitochondria of skeletal muscle from cachectic mice, and colocalized with the mitochondrial protein Tom20. Our data demonstrate the importance of mitophagy in cachexia. Understanding these changes will help contribute to defining treatments for cancer cachexia.', 'full_text': 'Bone metastases induce metabolic changes and mitophagy in mice.\\n\\nNEW FINDINGS\\nWhat is the central question of this study? Cachexia causes severe changes in skeletal muscle metabolism and function and is a key predictor of negative outcomes in cancer patients: what are the changes in whole animal energy metabolism and mitochondria in skeletal muscle? What is the main finding and its importance? There is decreased whole animal energy expenditure in mice with cachexia. They displayed highly dysmorphic mitochondria and mitophagy in skeletal muscle.\\n\\n\\nABSTRACT\\nCachexia causes changes in skeletal muscle metabolism. Mice with MDA-MB-231 breast cancer bone metastases and cachexia have decreased whole animal energy metabolism and increased skeletal muscle mitophagy. We examined whole animal energy metabolism by indirect calorimetry in mice with MDA-MB-231 breast cancer bone metastases, and showed decreased energy expenditure. We also examined skeletal muscle mitochondria and found that mitochondria in mice with MDA-MB-231 bone metastases are highly dysmorphic and have altered protein markers of mitochondrial biogenesis and dynamics. In addition, LC3B protein was increased in mitochondria of skeletal muscle from cachectic mice, and colocalized with the mitochondrial protein Tom20. Our data demonstrate the importance of mitophagy in cachexia. Understanding these changes will help contribute to defining treatments for cancer cachexia.', 'authors': [{'lastname': 'Wilcox-Hagerty', 'forename': 'Jenna', 'initials': 'J', 'identifier': '', 'affiliation': 'The Penn State College of Medicine, Department of Cellular and Molecular Physiology, Hershey, PA, USA.'}, {'lastname': 'Xu', 'forename': 'Haifang', 'initials': 'H', 'identifier': '', 'affiliation': 'The Penn State College of Medicine, Department of Cellular and Molecular Physiology, Hershey, PA, USA.'}, {'lastname': 'Hain', 'forename': 'Brian A', 'initials': 'BA', 'identifier': '', 'affiliation': 'The Penn State College of Medicine, Department of Cellular and Molecular Physiology, Hershey, PA, USA.'}, {'lastname': 'Arnold', 'forename': 'Amy C', 'initials': 'AC', 'identifier': '', 'affiliation': 'The Penn State College of Medicine, Department of Neural and Behavioral Sciences, Hershey, PA, USA.'}, {'lastname': 'Waning', 'forename': 'David L', 'initials': 'DL', 'identifier': '0000-0002-3858-7623', 'affiliation': 'The Penn State College of Medicine, Department of Cellular and Molecular Physiology, Hershey, PA, USA.'}], 'journal': 'Experimental physiology', 'pubdate': '2021-02'}}, {'_index': 'medline-faiss-hnsw-lexical', '_id': '12915066', '_score': 17.037426, '_source': {'pmid': '29219713', 'auto_id': 12915066, 'text': 'Liver transplantation. Preview How many months of documented abstinence are required before a patient with alcoholic cirrhosis can be considered for liver transplantation? What is the role of liver transplantation as treatment for primary hepatocellular cancer? What are the possible causes of elevated liver enzyme levels following transplantation? Which patients with neurologic injury are potential organ donors? The authors answer these and other questions primary care physicians may have about this lifesaving procedure.', 'full_text': 'Liver transplantation.\\n\\nPreview How many months of documented abstinence are required before a patient with alcoholic cirrhosis can be considered for liver transplantation? What is the role of liver transplantation as treatment for primary hepatocellular cancer? What are the possible causes of elevated liver enzyme levels following transplantation? Which patients with neurologic injury are potential organ donors? The authors answer these and other questions primary care physicians may have about this lifesaving procedure.', 'authors': [{'lastname': 'Gholson', 'forename': 'Charles F', 'initials': 'CF', 'identifier': '', 'affiliation': ''}, {'lastname': 'McDonald', 'forename': 'John', 'initials': 'J', 'identifier': '', 'affiliation': ''}, {'lastname': 'McMillan', 'forename': 'Robert', 'initials': 'R', 'identifier': '', 'affiliation': ''}], 'journal': 'Postgraduate medicine', 'pubdate': '1995-02'}}]}}\n"
     ]
    }
   ],
   "source": [
    "query_body = {\n",
    "    \"size\": 10,\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": \"What are the Cancer Causes\",\n",
    "            \"fields\": [\"text\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the query\n",
    "response = client_lexical.search(\n",
    "    index=\"medline-faiss-hnsw-lexical\",\n",
    "    body=query_body\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from qdrant_client.http import models\n",
    "\n",
    "\n",
    "# 3.145.52.195\n",
    "client_semantic = QdrantClient(host, port=6333, timeout = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the type of lexical indexing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_pmid = False\n",
    "\n",
    "if lexical_pmid:\n",
    "    index_name_lexical = 'medline-faiss-hnsw-lexical-pmid'\n",
    "else:\n",
    "    index_name_lexical ='medline-faiss-hnsw-lexical'\n",
    "\n",
    "coll_name_semantic = \"medline-faiss-hnsw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lcass\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lcass\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ensure that the necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, index_lexical:str = \"medline-faiss-hnsw-lexical\",lexical_pmid = False, index_name_semantic =\"medline-faiss-hnsw\", rescore = False, model=None, lexical_client=None, semantic_client=None, output_file_path=\"queries/queries.tsv\", stopwords=set([])):\n",
    "        self.index_lexical_name = index_lexical\n",
    "        self.index_name_semantic = index_name_semantic\n",
    "        # 2 index name (?)\n",
    "        self.model = model\n",
    "        #self.lexical_pmid = lexical_pmid\n",
    "        self.lexical_client = lexical_client\n",
    "        self.semantic_client = semantic_client\n",
    "        self.output_file_path = output_file_path\n",
    "        self.stop_words = stopwords\n",
    "        self.query_result = []\n",
    "        self.rescore = rescore\n",
    "        self.lexical_query = self.lexical_query_pmid if lexical_pmid else self.lexical_query\n",
    "    \n",
    "    def set_rescore(self, rescore):\n",
    "        self.rescore = rescore\n",
    "\n",
    "    def preprocess_query(self, query_str):\n",
    "        return ' '.join([word for word in word_tokenize(query_str) if word.lower() not in self.stop_words])\n",
    "\n",
    "    def save_results(self):\n",
    "        with open(self.output_file_path, \"w\") as file:\n",
    "            json.dump(self.query_result, file, indent=4)\n",
    "      \n",
    "    \n",
    "    def reorder_pmid(self, retrived_documents):\n",
    "        pmid_scores = {}\n",
    "        \n",
    "        # Iterate through the set data\n",
    "        for _, value in retrived_documents.items():\n",
    "            pmid = value['pmid']\n",
    "            score = value['score']\n",
    "            \n",
    "            # Check if pmid already exists in the dictionary\n",
    "            if pmid in pmid_scores:\n",
    "                pmid_scores[pmid] += score\n",
    "            else:\n",
    "                pmid_scores[pmid] = score\n",
    "           \n",
    "        return pmid_scores\n",
    "    \n",
    "    def lexical_query(self, query_str, limit=10):\n",
    "        if self.lexical_client == None:\n",
    "            raise ValueError(\"No Lexical client defined\")\n",
    "        \n",
    "        query = {\n",
    "                \"size\": limit,\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query_str,\n",
    "                        \"fields\": [\"text\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "       \n",
    "        results = self.lexical_client.search(index=self.index_lexical_name, body=query) \n",
    "        retrived_documents = {}\n",
    "        max_score = results['hits']['max_score']\n",
    "     \n",
    "        for hit in results[\"hits\"][\"hits\"]:\n",
    "            \n",
    "            pmid = hit[\"_source\"][\"pmid\"]\n",
    "            score = hit[\"_score\"]\n",
    "            auto_id = hit[\"_id\"]\n",
    "            \n",
    "            \n",
    "            \n",
    "            retrived_documents[auto_id] = {\n",
    "                \"score\": round(score/max_score, 5),\n",
    "                \"pmid\": pmid\n",
    "                }\n",
    "        \n",
    "        retrived_documents = self.reorder_pmid(retrived_documents)\n",
    "        return retrived_documents #adjust the return \n",
    "    \n",
    "    def lexical_query_pmid(self, query_str, limit=10):\n",
    "        #print(\"Lexical = \",query_str)\n",
    "        if self.lexical_client == None:\n",
    "            raise ValueError(\"No Lexical client defined\")\n",
    "        \n",
    "        query = {\n",
    "                \"size\": limit,\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query_str,\n",
    "                        \"fields\": [\"full_text\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        results = self.lexical_client.search(index=self.index_lexical_name, body=query) \n",
    "        \n",
    "        retrieved_documents = {}\n",
    "        max_score = results['hits']['max_score']\n",
    "        for hit in results[\"hits\"][\"hits\"]:\n",
    "            \n",
    "            pmid = hit[\"_source\"][\"pmid\"]\n",
    "            score = hit[\"_score\"] / max_score\n",
    "            \n",
    "            retrieved_documents[pmid] = score\n",
    "            \n",
    "        return retrieved_documents #adjust the return \n",
    "\n",
    "    def semantic_query(self, query, limit=10):\n",
    "        #print(\"semantic = \",query)\n",
    "        if self.semantic_client == None:\n",
    "            raise ValueError(\"No Semantic client defined\")\n",
    "        if self.model == None:\n",
    "            raise ValueError(\"No model defined\")\n",
    "        \n",
    "        query_vector = self.model.encode(query).tolist()\n",
    "    \n",
    "        search_params=models.SearchParams(\n",
    "            quantization=models.QuantizationSearchParams(rescore=self.rescore)\n",
    "            )\n",
    "        results = self.semantic_client.search(collection_name=self.index_name_semantic,query_vector=query_vector,search_params=search_params, limit=limit)\n",
    "    \n",
    "        #results = self.semantic_client.search(collection_name=self.index_name_semantic,query_vector=query_vector, limit=limit)\n",
    "        \n",
    "        retrived_documents = {}\n",
    "        max_score = None\n",
    "        for i,document in enumerate(results):\n",
    "            \n",
    "            pmid = document.payload['pmid']\n",
    "            score = document.score\n",
    "            if i == 0:\n",
    "                # first score is the max\n",
    "                max_score = score\n",
    "            retrived_documents[document.id] = { 'pmid': pmid, 'score': round(score / max_score, 5) } \n",
    "\n",
    "        retrived_documents = self.reorder_pmid(retrived_documents)\n",
    "        \n",
    "        return retrived_documents\n",
    "    \n",
    "\n",
    "    def hybrid_query(self, query_lexical, query_semantic, lex_parameter = 0.5, semantic_parameter = 0.5, limit=10):\n",
    "        if (lex_parameter + semantic_parameter) > 1:\n",
    "            raise ValueError(\"Uncorrect parameters for Hybrid Queries\")\n",
    "        lexical_results = self.lexical_query(query_lexical, limit = limit) \n",
    "        semantic_results = self.semantic_query(query_semantic, limit)\n",
    "        max_score = 0\n",
    "        retrived_documents = {}\n",
    "        \n",
    "        for lex_pmid in lexical_results:\n",
    "            score = lexical_results[lex_pmid] * lex_parameter\n",
    "            if lex_pmid in semantic_results:\n",
    "                score += semantic_results[lex_pmid] * semantic_parameter\n",
    "\n",
    "            retrived_documents[lex_pmid] = score\n",
    "            max_score = max(max_score, score)\n",
    "            \n",
    "\n",
    "        for semantic_pmid in semantic_results:\n",
    "            if semantic_pmid not in lexical_results:\n",
    "                score = semantic_results[semantic_pmid] * semantic_parameter\n",
    "                retrived_documents[semantic_pmid] = score\n",
    "                max_score = max(max_score, score)\n",
    "                \n",
    "        return retrived_documents # just to have a starting point\n",
    "\n",
    "\n",
    "    def execute_query(self, query_str, query_type='lexical', lex_parameter = 0.5, semantic_parameter = 0.5,limit = 10,save = True, stopwords_preprocessing=True):\n",
    "        #print(\"Before = \",query_str)\n",
    "        text_query = self.preprocess_query(query_str) if stopwords_preprocessing else query_str\n",
    "        \n",
    "        if query_type == 'lexical':\n",
    "            results = self.lexical_query(text_query, limit=limit) \n",
    "        \n",
    "        elif query_type == 'semantic':\n",
    "            results = self.semantic_query(query_str, limit=limit)\n",
    "\n",
    "        elif query_type == 'hybrid':\n",
    "            results = self.hybrid_query(text_query, query_str, lex_parameter, semantic_parameter, limit=limit)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid query type specified. Choose 'lexical', 'semantic', or 'hybrid'.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        document_retrived = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "        document_retrived = document_retrived[:limit+1] # in the hybrid search we can return more documents\n",
    "        #print(\"Results \", document_retrived)\n",
    "        if save:\n",
    "            self.process_results(document_retrived, query_str, query_type)\n",
    "\n",
    "        return document_retrived\n",
    "    \n",
    "    # needs to be rewrited\n",
    "    def process_results(self, results, query_str,query_type):\n",
    "        \n",
    "        retrieved_documents = []\n",
    "        for element in results:\n",
    "            \n",
    "            pmid,_ = element\n",
    "            query = {\n",
    "                    \"query\": {\n",
    "                        \"term\": {\n",
    "                        \"pmid\": int(pmid)\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            results = self.lexical_client.search(index=self.index_lexical_name, body=query) \n",
    "            full_text = results['hits']['hits'][0][\"_source\"]['full_text']\n",
    "            pmid = results['hits']['hits'][0][\"_source\"]['pmid']\n",
    "\n",
    "            retrieved_documents.append({\n",
    "                \"pmid\": pmid,\n",
    "                \"text\": full_text\n",
    "            })\n",
    "\n",
    "        dict_to_save = {'query': query_str, 'query_type': query_type, 'abstracts' : retrieved_documents}\n",
    "        self.query_result.append(dict_to_save)  \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_parser = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10797929', 0.5),\n",
       " ('15877281', 0.5),\n",
       " ('37560515', 0.49687),\n",
       " ('29597095', 0.494725),\n",
       " ('20870045', 0.49326),\n",
       " ('29922639', 0.49277),\n",
       " ('9462748', 0.490925),\n",
       " ('22303795', 0.49066),\n",
       " ('24914010', 0.490565),\n",
       " ('19332160', 0.49053),\n",
       " ('22106036', 0.49053)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_parser.execute_query(query_str=\"Which gene is responsible for disfunction in speech for children?\", query_type='hybrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the evaluation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5049\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "evaluation_file = 'training12b_new.json'\n",
    "\n",
    "with open(evaluation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data['questions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def clean_documents(documents):\n",
    "    output_documents = set()\n",
    "    for doc in documents:\n",
    "        output_documents.add((doc.replace(\"http://www.ncbi.nlm.nih.gov/pubmed/\",\"\")))\n",
    "    return output_documents\n",
    "\n",
    "def average_precision(retrived_doc, true_doc):\n",
    "    # Initialize variables\n",
    "    precision_sum = 0\n",
    "    num_retrieved_docs = 0\n",
    "    \n",
    "    # Calculate precision at each relevant document position\n",
    "    for i, retrived in enumerate(retrived_doc, start=1):\n",
    "        pmid,_ = retrived\n",
    "        if pmid in true_doc:  # Check if the document is relevant\n",
    "            num_retrieved_docs += 1\n",
    "            precision_sum += num_retrieved_docs / i  # Calculate precision at cutoff i\n",
    "\n",
    "    # Calculate average precision\n",
    "    if num_retrieved_docs == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return precision_sum / num_retrieved_docs\n",
    "\n",
    "\n",
    "def evaluation(query_parser, data, query_type,alpha=0.5, beta=0.5, stopwords_preprocessing = False, path = \"query_result.json\"):\n",
    "    avg_precisions_sum = [] # sum all average precision and divide with number of queries \n",
    "    precisions_sum = []\n",
    "    queries_time = []\n",
    "    for i,question in enumerate(data['questions']):\n",
    "        dict_to_save = {}\n",
    "        query = question['body']\n",
    "        dict_to_save['query'] = query\n",
    "        dict_to_save['query_type'] = query_type\n",
    "        relevant_documents = clean_documents(question['documents'])\n",
    "        start_time = time.time()\n",
    "        results = query_parser.execute_query(query,query_type = query_type, lex_parameter = alpha, semantic_parameter = beta,limit = len(relevant_documents), save=False, stopwords_preprocessing = stopwords_preprocessing)\n",
    "        queries_time.append(time.time() - start_time)\n",
    "        \n",
    "        #results = [('20598273',1), ('4',1), ('6650562',1), ('2',1),('21995290',1),('15617541',1),('23001136',1),('8896569',1), ('12239580',1)]\n",
    "        dict_to_save['true_documents'] = list(relevant_documents)\n",
    "        dict_to_save['retrieved_documents'] = results\n",
    "       \n",
    "\n",
    "    \n",
    "        number_retrieved_documents = 0\n",
    "        for pmid,_ in results:\n",
    "            if pmid in relevant_documents:\n",
    "                number_retrieved_documents +=1\n",
    "\n",
    "        precision = number_retrieved_documents / len(relevant_documents)\n",
    "        recall = number_retrieved_documents / len(relevant_documents)\n",
    "        avg_precision = average_precision(results, relevant_documents)\n",
    "        \n",
    "        precisions_sum.append(precision)\n",
    "        #recalls.append(recall)\n",
    "        \n",
    "        avg_precisions_sum.append(avg_precision)\n",
    "        \n",
    "        dict_to_save['precision'] = precision\n",
    "        #dict_to_save['recall'] = recall\n",
    "        dict_to_save['avg_precision'] = avg_precision\n",
    "        with open(path, 'a') as output_file:\n",
    "            output_file.write(json.dumps(dict_to_save) + '\\n')\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f\"Analyzed {i+1} queries\")\n",
    "            print(\"Actual Results...\")\n",
    "            print(f\"Mean precision = {np.mean(precisions_sum):.3f}\")\n",
    "            #print(f\"Mean recall = {np.mean(recalls):.3f}\")\n",
    "            print(f\"Mean Average Precision = {np.mean(avg_precisions_sum):.3f}\")\n",
    "            print(f\"Mean Time needed to execute a query = {np.mean(queries_time):.3f}\")\n",
    "    print(\"FINAL RESULTS \")\n",
    "    print(f\"Mean precision = {np.mean(precisions_sum):.3f}\")\n",
    "    #print(f\"Mean recall = {np.mean(recalls):.3f}\")\n",
    "    print(f\"Mean Average Precision = {np.mean(avg_precisions_sum):.3f}\")\n",
    "    print(f\"Mean Time needed to execute a query = {np.mean(queries_time):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Lexical Auto-id Stopwords False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.222\n",
      "Mean Average Precision = 0.387\n",
      "Mean Time needed to execute a query = 0.206\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.230\n",
      "Mean Average Precision = 0.382\n",
      "Mean Time needed to execute a query = 0.204\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.227\n",
      "Mean Average Precision = 0.373\n",
      "Mean Time needed to execute a query = 0.204\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.239\n",
      "Mean Average Precision = 0.382\n",
      "Mean Time needed to execute a query = 0.203\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.261\n",
      "Mean Average Precision = 0.400\n",
      "Mean Time needed to execute a query = 0.200\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.263\n",
      "Mean Average Precision = 0.393\n",
      "Mean Time needed to execute a query = 0.198\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.268\n",
      "Mean Average Precision = 0.392\n",
      "Mean Time needed to execute a query = 0.196\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.273\n",
      "Mean Average Precision = 0.397\n",
      "Mean Time needed to execute a query = 0.197\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.279\n",
      "Mean Average Precision = 0.400\n",
      "Mean Time needed to execute a query = 0.196\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.277\n",
      "Mean Average Precision = 0.398\n",
      "Mean Time needed to execute a query = 0.197\n",
      "Mean precision = 0.277\n",
      "Mean Average Precision = 0.398\n",
      "Mean Time needed to execute a query = 0.197\n"
     ]
    }
   ],
   "source": [
    "evaluation(query_parser,data, query_type=\"lexical\", path = \"lexical_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Lexical Pmid Stopwords False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical-pmid\"\n",
    "query_parser_pmid = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=True, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.224\n",
      "Mean Average Precision = 0.394\n",
      "Mean Time needed to execute a query = 0.226\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.233\n",
      "Mean Average Precision = 0.388\n",
      "Mean Time needed to execute a query = 0.219\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.230\n",
      "Mean Average Precision = 0.379\n",
      "Mean Time needed to execute a query = 0.215\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.243\n",
      "Mean Average Precision = 0.388\n",
      "Mean Time needed to execute a query = 0.212\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.265\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.208\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.268\n",
      "Mean Average Precision = 0.399\n",
      "Mean Time needed to execute a query = 0.204\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.272\n",
      "Mean Average Precision = 0.396\n",
      "Mean Time needed to execute a query = 0.201\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.278\n",
      "Mean Average Precision = 0.401\n",
      "Mean Time needed to execute a query = 0.199\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.284\n",
      "Mean Average Precision = 0.405\n",
      "Mean Time needed to execute a query = 0.198\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.282\n",
      "Mean Average Precision = 0.403\n",
      "Mean Time needed to execute a query = 0.198\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.283\n",
      "Mean Average Precision = 0.404\n",
      "Mean Time needed to execute a query = 0.198\n"
     ]
    }
   ],
   "source": [
    "evaluation(query_parser_pmid,data, query_type=\"lexical\", path = \"lexical_results_pmid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result lexical pmid with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'why', 'didn', 'd', 'of', 'their', 'any', 'on', 'few', 'me', 'having', 'o', 'such', 'through', 'shan', 'll', 'during', 're', 'hadn', 'this', 'should', 'mustn', \"couldn't\", 'yourselves', 'so', 'wouldn', 'who', 'y', 'before', 'only', 'its', \"you're\", \"isn't\", 'mightn', 'be', 'nor', 'further', 'and', 'herself', 'where', 'until', 'aren', 'an', 'himself', 'out', 'our', 'at', 'am', 'most', \"that'll\", 'needn', 'after', 'will', 'other', 'you', \"hasn't\", \"wouldn't\", \"hadn't\", 'whom', \"shouldn't\", 'shouldn', 'more', \"she's\", \"doesn't\", 'which', 'doing', 'very', 'i', 'his', 'down', 'being', 'by', 'above', \"wasn't\", 'hasn', \"mightn't\", 'yourself', 'do', 'weren', 'did', 'these', 'hers', 'for', 'him', \"it's\", 'here', \"don't\", \"won't\", 'yours', 'to', 'them', 'she', 'is', 'both', \"needn't\", 'with', 'haven', 'he', 'a', 'doesn', \"weren't\", 'm', 'ourselves', \"aren't\", 'does', 'themselves', 'below', 'as', 'the', 'couldn', 'my', 'not', 'we', 'were', 'has', 'when', 'wasn', 'are', 'but', 'about', 'in', \"mustn't\", 'into', 'now', 'then', \"shan't\", 'up', 'that', 'while', 'all', 'had', 'her', 'over', \"should've\", \"didn't\", 'than', 'own', 'isn', 'won', 'ain', 'myself', 'they', 'or', 'because', 'same', 'don', \"you'll\", 'under', 'ours', 'theirs', 'can', 'how', 've', \"haven't\", 'just', 'between', 'some', \"you'd\", 'too', 'each', 'your', 'those', 'no', 'what', 'have', 'there', 'itself', 'it', 'ma', 'again', \"you've\", 'from', 's', 'once', 'off', 't', 'was', 'against', 'if', 'been'}\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical-pmid\"\n",
    "lexical_pmid = True\n",
    "query_parser_stopwords = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic, stopwords=english_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.234\n",
      "Mean Average Precision = 0.412\n",
      "Mean Time needed to execute a query = 0.203\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.240\n",
      "Mean Average Precision = 0.399\n",
      "Mean Time needed to execute a query = 0.202\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.239\n",
      "Mean Average Precision = 0.391\n",
      "Mean Time needed to execute a query = 0.202\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.250\n",
      "Mean Average Precision = 0.399\n",
      "Mean Time needed to execute a query = 0.205\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.271\n",
      "Mean Average Precision = 0.416\n",
      "Mean Time needed to execute a query = 0.200\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.274\n",
      "Mean Average Precision = 0.409\n",
      "Mean Time needed to execute a query = 0.194\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.277\n",
      "Mean Average Precision = 0.405\n",
      "Mean Time needed to execute a query = 0.189\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.282\n",
      "Mean Average Precision = 0.409\n",
      "Mean Time needed to execute a query = 0.188\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.288\n",
      "Mean Average Precision = 0.412\n",
      "Mean Time needed to execute a query = 0.188\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.286\n",
      "Mean Average Precision = 0.410\n",
      "Mean Time needed to execute a query = 0.189\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.287\n",
      "Mean Average Precision = 0.411\n",
      "Mean Time needed to execute a query = 0.189\n"
     ]
    }
   ],
   "source": [
    "evaluation(query_parser_stopwords,data, query_type=\"lexical\", path = \"lex_results_stopwords.json\",stopwords_preprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result lexical autoid stopword True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.228\n",
      "Mean Average Precision = 0.402\n",
      "Mean Time needed to execute a query = 0.258\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.235\n",
      "Mean Average Precision = 0.391\n",
      "Mean Time needed to execute a query = 0.258\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.234\n",
      "Mean Average Precision = 0.381\n",
      "Mean Time needed to execute a query = 0.256\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.245\n",
      "Mean Average Precision = 0.389\n",
      "Mean Time needed to execute a query = 0.259\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.267\n",
      "Mean Average Precision = 0.408\n",
      "Mean Time needed to execute a query = 0.250\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.270\n",
      "Mean Average Precision = 0.401\n",
      "Mean Time needed to execute a query = 0.240\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.274\n",
      "Mean Average Precision = 0.400\n",
      "Mean Time needed to execute a query = 0.230\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.279\n",
      "Mean Average Precision = 0.403\n",
      "Mean Time needed to execute a query = 0.226\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.284\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.224\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.281\n",
      "Mean Average Precision = 0.404\n",
      "Mean Time needed to execute a query = 0.225\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.282\n",
      "Mean Average Precision = 0.404\n",
      "Mean Time needed to execute a query = 0.225\n"
     ]
    }
   ],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical\"\n",
    "lexical_pmid = False\n",
    "query_parser_stopwords = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic, stopwords=english_stopwords)\n",
    "evaluation(query_parser_stopwords,data, query_type=\"lexical\", path = \"lex_results_stopwords_auto_id.json\",stopwords_preprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Semantic without rescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.134\n",
      "Mean Average Precision = 0.276\n",
      "Mean Time needed to execute a query = 0.261\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.126\n",
      "Mean Average Precision = 0.265\n",
      "Mean Time needed to execute a query = 0.262\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.126\n",
      "Mean Average Precision = 0.262\n",
      "Mean Time needed to execute a query = 0.261\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.130\n",
      "Mean Average Precision = 0.268\n",
      "Mean Time needed to execute a query = 0.260\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.137\n",
      "Mean Average Precision = 0.272\n",
      "Mean Time needed to execute a query = 0.254\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.139\n",
      "Mean Average Precision = 0.266\n",
      "Mean Time needed to execute a query = 0.252\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.138\n",
      "Mean Average Precision = 0.259\n",
      "Mean Time needed to execute a query = 0.251\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.140\n",
      "Mean Average Precision = 0.259\n",
      "Mean Time needed to execute a query = 0.249\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.140\n",
      "Mean Average Precision = 0.256\n",
      "Mean Time needed to execute a query = 0.246\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.140\n",
      "Mean Average Precision = 0.256\n",
      "Mean Time needed to execute a query = 0.245\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.140\n",
      "Mean Average Precision = 0.257\n",
      "Mean Time needed to execute a query = 0.245\n"
     ]
    }
   ],
   "source": [
    "evaluation(query_parser,data, query_type=\"semantic\", path = \"semantic_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Semantic with rescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.135\n",
      "Mean Average Precision = 0.282\n",
      "Mean Time needed to execute a query = 0.369\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.129\n",
      "Mean Average Precision = 0.268\n",
      "Mean Time needed to execute a query = 0.358\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.130\n",
      "Mean Average Precision = 0.266\n",
      "Mean Time needed to execute a query = 0.352\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.134\n",
      "Mean Average Precision = 0.273\n",
      "Mean Time needed to execute a query = 0.349\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.139\n",
      "Mean Average Precision = 0.277\n",
      "Mean Time needed to execute a query = 0.336\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.143\n",
      "Mean Average Precision = 0.271\n",
      "Mean Time needed to execute a query = 0.323\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.142\n",
      "Mean Average Precision = 0.263\n",
      "Mean Time needed to execute a query = 0.310\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.144\n",
      "Mean Average Precision = 0.263\n",
      "Mean Time needed to execute a query = 0.303\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.143\n",
      "Mean Average Precision = 0.260\n",
      "Mean Time needed to execute a query = 0.299\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.143\n",
      "Mean Average Precision = 0.259\n",
      "Mean Time needed to execute a query = 0.297\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.144\n",
      "Mean Average Precision = 0.260\n",
      "Mean Time needed to execute a query = 0.297\n"
     ]
    }
   ],
   "source": [
    "query_parser.set_rescore(True)\n",
    "evaluation(query_parser,data, query_type=\"semantic\", path = \"semantic_results.json\")\n",
    "query_parser.set_rescore(False) # re insert the rescore to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Hybrid with lexical autoid Stopwords False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.184\n",
      "Mean Average Precision = 0.388\n",
      "Mean Time needed to execute a query = 0.703\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.181\n",
      "Mean Average Precision = 0.381\n",
      "Mean Time needed to execute a query = 0.691\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.180\n",
      "Mean Average Precision = 0.368\n",
      "Mean Time needed to execute a query = 0.697\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.189\n",
      "Mean Average Precision = 0.380\n",
      "Mean Time needed to execute a query = 0.698\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.211\n",
      "Mean Average Precision = 0.396\n",
      "Mean Time needed to execute a query = 0.684\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.220\n",
      "Mean Average Precision = 0.393\n",
      "Mean Time needed to execute a query = 0.666\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.230\n",
      "Mean Average Precision = 0.393\n",
      "Mean Time needed to execute a query = 0.648\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.238\n",
      "Mean Average Precision = 0.399\n",
      "Mean Time needed to execute a query = 0.641\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.244\n",
      "Mean Average Precision = 0.402\n",
      "Mean Time needed to execute a query = 0.636\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.245\n",
      "Mean Average Precision = 0.402\n",
      "Mean Time needed to execute a query = 0.638\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.245\n",
      "Mean Average Precision = 0.403\n",
      "Mean Time needed to execute a query = 0.638\n"
     ]
    }
   ],
   "source": [
    "evaluation(query_parser, data, query_type=\"hybrid\", path = \"hybrid_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Hybrid with lexical pmid Stopwords False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.184\n",
      "Mean Average Precision = 0.392\n",
      "Mean Time needed to execute a query = 0.701\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.180\n",
      "Mean Average Precision = 0.383\n",
      "Mean Time needed to execute a query = 0.695\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.180\n",
      "Mean Average Precision = 0.370\n",
      "Mean Time needed to execute a query = 0.690\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.189\n",
      "Mean Average Precision = 0.384\n",
      "Mean Time needed to execute a query = 0.689\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.211\n",
      "Mean Average Precision = 0.399\n",
      "Mean Time needed to execute a query = 0.673\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.221\n",
      "Mean Average Precision = 0.397\n",
      "Mean Time needed to execute a query = 0.652\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.229\n",
      "Mean Average Precision = 0.396\n",
      "Mean Time needed to execute a query = 0.632\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.238\n",
      "Mean Average Precision = 0.402\n",
      "Mean Time needed to execute a query = 0.624\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.244\n",
      "Mean Average Precision = 0.405\n",
      "Mean Time needed to execute a query = 0.619\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.246\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.619\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.246\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.619\n"
     ]
    }
   ],
   "source": [
    "evaluation(query_parser_pmid, data, query_type=\"hybrid\", path = \"hybrid_results_pmid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Hybrid pmid with stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.190\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.501\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.185\n",
      "Mean Average Precision = 0.390\n",
      "Mean Time needed to execute a query = 0.499\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.184\n",
      "Mean Average Precision = 0.378\n",
      "Mean Time needed to execute a query = 0.498\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.193\n",
      "Mean Average Precision = 0.390\n",
      "Mean Time needed to execute a query = 0.498\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.214\n",
      "Mean Average Precision = 0.404\n",
      "Mean Time needed to execute a query = 0.484\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.224\n",
      "Mean Average Precision = 0.402\n",
      "Mean Time needed to execute a query = 0.468\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.231\n",
      "Mean Average Precision = 0.399\n",
      "Mean Time needed to execute a query = 0.454\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.241\n",
      "Mean Average Precision = 0.405\n",
      "Mean Time needed to execute a query = 0.450\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.247\n",
      "Mean Average Precision = 0.408\n",
      "Mean Time needed to execute a query = 0.448\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.248\n",
      "Mean Average Precision = 0.409\n",
      "Mean Time needed to execute a query = 0.451\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.249\n",
      "Mean Average Precision = 0.409\n",
      "Mean Time needed to execute a query = 0.451\n"
     ]
    }
   ],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical-pmid\"\n",
    "lexical_pmid = True\n",
    "query_parser_stopwords = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic, stopwords=english_stopwords)\n",
    "evaluation(query_parser_stopwords, data, query_type=\"hybrid\",alpha=0.5, beta=0.5, path = \"hybrid_results_pmid_stopwords.json\", stopwords_preprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Hybrid autoid with stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.192\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.652\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.187\n",
      "Mean Average Precision = 0.387\n",
      "Mean Time needed to execute a query = 0.642\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.185\n",
      "Mean Average Precision = 0.375\n",
      "Mean Time needed to execute a query = 0.625\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.194\n",
      "Mean Average Precision = 0.386\n",
      "Mean Time needed to execute a query = 0.625\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.215\n",
      "Mean Average Precision = 0.403\n",
      "Mean Time needed to execute a query = 0.604\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.225\n",
      "Mean Average Precision = 0.400\n",
      "Mean Time needed to execute a query = 0.578\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.234\n",
      "Mean Average Precision = 0.399\n",
      "Mean Time needed to execute a query = 0.556\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.242\n",
      "Mean Average Precision = 0.405\n",
      "Mean Time needed to execute a query = 0.546\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.248\n",
      "Mean Average Precision = 0.407\n",
      "Mean Time needed to execute a query = 0.540\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.248\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.542\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.249\n",
      "Mean Average Precision = 0.407\n",
      "Mean Time needed to execute a query = 0.541\n"
     ]
    }
   ],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical\"\n",
    "lexical_pmid = False\n",
    "query_parser_stopwords = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic, stopwords=english_stopwords)\n",
    "evaluation(query_parser_stopwords, data, query_type=\"hybrid\",alpha=0.5, beta=0.5, path = \"hybrid_results_autoid_stopwords.json\", stopwords_preprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Hybrid with rescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.192\n",
      "Mean Average Precision = 0.403\n",
      "Mean Time needed to execute a query = 0.538\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.189\n",
      "Mean Average Precision = 0.389\n",
      "Mean Time needed to execute a query = 0.542\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.187\n",
      "Mean Average Precision = 0.378\n",
      "Mean Time needed to execute a query = 0.542\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.197\n",
      "Mean Average Precision = 0.392\n",
      "Mean Time needed to execute a query = 0.548\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.218\n",
      "Mean Average Precision = 0.405\n",
      "Mean Time needed to execute a query = 0.533\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.228\n",
      "Mean Average Precision = 0.403\n",
      "Mean Time needed to execute a query = 0.516\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.235\n",
      "Mean Average Precision = 0.400\n",
      "Mean Time needed to execute a query = 0.501\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.244\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.493\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.250\n",
      "Mean Average Precision = 0.409\n",
      "Mean Time needed to execute a query = 0.489\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.251\n",
      "Mean Average Precision = 0.409\n",
      "Mean Time needed to execute a query = 0.489\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.252\n",
      "Mean Average Precision = 0.410\n",
      "Mean Time needed to execute a query = 0.489\n"
     ]
    }
   ],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical-pmid\"\n",
    "lexical_pmid = True\n",
    "query_parser_stopwords = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic, stopwords=english_stopwords)\n",
    "query_parser_stopwords.set_rescore(True)\n",
    "evaluation(query_parser_stopwords, data, query_type=\"hybrid\",alpha=0.5, beta=0.5, path = \"hybrid_results_pmid_recore.json\", stopwords_preprocessing=True)\n",
    "query_parser_stopwords.set_rescore(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Hybrid with alpha 0.7 and Beta 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.246\n",
      "Mean Average Precision = 0.412\n",
      "Mean Time needed to execute a query = 0.560\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.253\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.543\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.251\n",
      "Mean Average Precision = 0.397\n",
      "Mean Time needed to execute a query = 0.539\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.262\n",
      "Mean Average Precision = 0.408\n",
      "Mean Time needed to execute a query = 0.539\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.285\n",
      "Mean Average Precision = 0.424\n",
      "Mean Time needed to execute a query = 0.526\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.291\n",
      "Mean Average Precision = 0.420\n",
      "Mean Time needed to execute a query = 0.512\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.296\n",
      "Mean Average Precision = 0.418\n",
      "Mean Time needed to execute a query = 0.496\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.303\n",
      "Mean Average Precision = 0.422\n",
      "Mean Time needed to execute a query = 0.489\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.308\n",
      "Mean Average Precision = 0.425\n",
      "Mean Time needed to execute a query = 0.487\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.307\n",
      "Mean Average Precision = 0.425\n",
      "Mean Time needed to execute a query = 0.489\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.308\n",
      "Mean Average Precision = 0.425\n",
      "Mean Time needed to execute a query = 0.489\n"
     ]
    }
   ],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical-pmid\"\n",
    "lexical_pmid = True\n",
    "query_parser_stopwords = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic, stopwords=english_stopwords)\n",
    "query_parser_stopwords.set_rescore(True)\n",
    "evaluation(query_parser_stopwords, data, query_type=\"hybrid\",alpha=0.7, beta=0.3, path = \"hybrid_results_pmid_recore.json\", stopwords_preprocessing=True)\n",
    "query_parser_stopwords.set_rescore(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Hybrid with 0.8 and 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.246\n",
      "Mean Average Precision = 0.413\n",
      "Mean Time needed to execute a query = 0.551\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.253\n",
      "Mean Average Precision = 0.406\n",
      "Mean Time needed to execute a query = 0.561\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.251\n",
      "Mean Average Precision = 0.398\n",
      "Mean Time needed to execute a query = 0.550\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.262\n",
      "Mean Average Precision = 0.409\n",
      "Mean Time needed to execute a query = 0.549\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.285\n",
      "Mean Average Precision = 0.424\n",
      "Mean Time needed to execute a query = 0.536\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.292\n",
      "Mean Average Precision = 0.420\n",
      "Mean Time needed to execute a query = 0.522\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.296\n",
      "Mean Average Precision = 0.418\n",
      "Mean Time needed to execute a query = 0.508\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.303\n",
      "Mean Average Precision = 0.423\n",
      "Mean Time needed to execute a query = 0.502\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.308\n",
      "Mean Average Precision = 0.425\n",
      "Mean Time needed to execute a query = 0.499\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.307\n",
      "Mean Average Precision = 0.425\n",
      "Mean Time needed to execute a query = 0.500\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.308\n",
      "Mean Average Precision = 0.425\n",
      "Mean Time needed to execute a query = 0.500\n"
     ]
    }
   ],
   "source": [
    "index_name_lexical = \"medline-faiss-hnsw-lexical-pmid\"\n",
    "lexical_pmid = True\n",
    "query_parser_stopwords = QueryProcessor(index_lexical=index_name_lexical, lexical_pmid=lexical_pmid, index_name_semantic = coll_name_semantic, model= model, lexical_client=client_lexical, semantic_client=client_semantic, stopwords=english_stopwords)\n",
    "query_parser_stopwords.set_rescore(True)\n",
    "evaluation(query_parser_stopwords, data, query_type=\"hybrid\",alpha=0.8, beta=0.2, path = \"hybrid_results_pmid_recore.json\", stopwords_preprocessing=True)\n",
    "query_parser_stopwords.set_rescore(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation based on PubMed website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id list =  ['11076767', '30388611', '30661986', '15094110', '38284126', '12666201', '36997062', '22242013', '22937083', '19297413']\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "# Always tell NCBI who you are (your email address)\n",
    "Entrez.email = \"lcassano00@gmail.com\"\n",
    "def search_pubmed(query, limit = 10, mesh=True):\n",
    "    if not mesh:\n",
    "        query += \"[Title/Abstract]\"\n",
    "    # Use Entrez.esearch to search for articles matching the query in PubMed\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=limit, sort=\"relevance\",)\n",
    "    \n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    # Get the list of Ids returned by the search\n",
    "    id_list = record[\"IdList\"]\n",
    "    return id_list\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    # Use Entrez.efetch to get the article details from the list of Ids\n",
    "    ids = ','.join(id_list)\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=ids, retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return records\n",
    "# Example usage\n",
    "\n",
    "query = \"Is the protein Papilin secreted?\"\n",
    "id_list = search_pubmed(query)\n",
    "print(\"Id list = \",id_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id list =  ['11076767', '38284126', '15094122', '21784067', '7515725']\n"
     ]
    }
   ],
   "source": [
    "query = \"Is the protein Papilin secreted?\"\n",
    "id_list = search_pubmed(query, mesh=False)\n",
    "print(\"Id list = \",id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_pubmed(query_type, data,mesh=True,path = \"query_result.json\"):\n",
    "    avg_precisions_sum = [] # sum all average precision and divide with number of queries \n",
    "    precisions_sum = []\n",
    "    queries_time = []\n",
    "    for i,question in enumerate(data['questions']):\n",
    "        dict_to_save = {}\n",
    "        query = question['body']\n",
    "        dict_to_save['query'] = query\n",
    "        dict_to_save['query_type'] = query_type\n",
    "        relevant_documents = clean_documents(question['documents'])\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = search_pubmed(query, limit = len(relevant_documents),mesh=mesh)\n",
    "        queries_time.append(time.time() - start_time)\n",
    "        \n",
    "        dict_to_save['true_documents'] = list(relevant_documents)\n",
    "        dict_to_save['retrieved_documents'] = results\n",
    "       \n",
    "        number_retrieved_documents = 0\n",
    "        for pmid in results:\n",
    "            if pmid in relevant_documents:\n",
    "                number_retrieved_documents +=1\n",
    "\n",
    "        precision = number_retrieved_documents / len(relevant_documents)\n",
    "        recall = number_retrieved_documents / len(relevant_documents)\n",
    "        avg_precision = average_precision(results, relevant_documents)\n",
    "       \n",
    "        precisions_sum.append(precision)\n",
    "        #recalls.append(recall)\n",
    "        \n",
    "        avg_precisions_sum.append(avg_precision)\n",
    "        \n",
    "        dict_to_save['precision'] = precision\n",
    "        #dict_to_save['recall'] = recall\n",
    "        dict_to_save['avg_precision'] = avg_precision\n",
    "        with open(path, 'a') as output_file:\n",
    "            output_file.write(json.dumps(dict_to_save) + '\\n')\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f\"Analyzed {i+1} queries\")\n",
    "            print(\"Actual Results...\")\n",
    "            print(f\"Mean precision = {np.mean(precisions_sum):.3f}\")\n",
    "            #print(f\"Mean recall = {np.mean(recalls):.3f}\")\n",
    "            print(f\"Mean Average Precision = {np.mean(avg_precisions_sum):.3f}\")\n",
    "            print(f\"Mean Time needed to execute a query = {np.mean(queries_time):.3f}\")\n",
    "    print(\"FINAL RESULTS \")\n",
    "    print(f\"Mean precision = {np.mean(precisions_sum):.3f}\")\n",
    "    #print(f\"Mean recall = {np.mean(recalls):.3f}\")\n",
    "    print(f\"Mean Average Precision = {np.mean(avg_precisions_sum):.3f}\")\n",
    "    print(f\"Mean Time needed to execute a query = {np.mean(queries_time):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pubmed with Mesh Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mesh terms are applied automatically by PubMed website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.087\n",
      "Mean Average Precision = 0.172\n",
      "Mean Time needed to execute a query = 0.744\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.086\n",
      "Mean Average Precision = 0.175\n",
      "Mean Time needed to execute a query = 0.748\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.084\n",
      "Mean Average Precision = 0.165\n",
      "Mean Time needed to execute a query = 0.746\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.093\n",
      "Mean Average Precision = 0.174\n",
      "Mean Time needed to execute a query = 0.745\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.108\n",
      "Mean Average Precision = 0.188\n",
      "Mean Time needed to execute a query = 0.741\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.109\n",
      "Mean Average Precision = 0.184\n",
      "Mean Time needed to execute a query = 0.743\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.109\n",
      "Mean Average Precision = 0.180\n",
      "Mean Time needed to execute a query = 0.739\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.111\n",
      "Mean Average Precision = 0.181\n",
      "Mean Time needed to execute a query = 0.736\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.116\n",
      "Mean Average Precision = 0.186\n",
      "Mean Time needed to execute a query = 0.741\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.120\n",
      "Mean Average Precision = 0.191\n",
      "Mean Time needed to execute a query = 0.742\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.120\n",
      "Mean Average Precision = 0.191\n",
      "Mean Time needed to execute a query = 0.742\n"
     ]
    }
   ],
   "source": [
    "evaluation_pubmed(data=data, query_type=\"PubMed website\", path=\"Pubmed_mesh.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation PubMed without mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.058\n",
      "Mean Average Precision = 0.128\n",
      "Mean Time needed to execute a query = 0.775\n",
      "Analyzed 1000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.059\n",
      "Mean Average Precision = 0.131\n",
      "Mean Time needed to execute a query = 0.739\n",
      "Analyzed 1500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.056\n",
      "Mean Average Precision = 0.124\n",
      "Mean Time needed to execute a query = 0.723\n",
      "Analyzed 2000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.068\n",
      "Mean Average Precision = 0.137\n",
      "Mean Time needed to execute a query = 0.719\n",
      "Analyzed 2500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.081\n",
      "Mean Average Precision = 0.150\n",
      "Mean Time needed to execute a query = 0.714\n",
      "Analyzed 3000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.083\n",
      "Mean Average Precision = 0.146\n",
      "Mean Time needed to execute a query = 0.708\n",
      "Analyzed 3500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.085\n",
      "Mean Average Precision = 0.145\n",
      "Mean Time needed to execute a query = 0.705\n",
      "Analyzed 4000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.086\n",
      "Mean Average Precision = 0.146\n",
      "Mean Time needed to execute a query = 0.707\n",
      "Analyzed 4500 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.090\n",
      "Mean Average Precision = 0.150\n",
      "Mean Time needed to execute a query = 0.702\n",
      "Analyzed 5000 queries\n",
      "Actual Results...\n",
      "Mean precision = 0.092\n",
      "Mean Average Precision = 0.153\n",
      "Mean Time needed to execute a query = 0.698\n",
      "FINAL RESULTS \n",
      "Mean precision = 0.092\n",
      "Mean Average Precision = 0.153\n",
      "Mean Time needed to execute a query = 0.698\n"
     ]
    }
   ],
   "source": [
    "evaluation_pubmed(data=data, query_type=\"PubMed website no mash\", mesh = False, path=\"Pubmed_no_mesh.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
